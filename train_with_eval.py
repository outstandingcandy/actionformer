# python imports
import argparse
import os
import time
import datetime
import json
from pprint import pprint

# torch imports
import torch
import torch.nn as nn
import torch.utils.data
# for visualization
from torch.utils.tensorboard import SummaryWriter

# our code
from libs.core import load_config
from libs.datasets import make_dataset, make_data_loader
from libs.modeling import make_meta_arch
from libs.utils import (train_one_epoch, valid_one_epoch, ANETdetection,
                        save_checkpoint, make_optimizer, make_scheduler,
                        fix_random_seed, ModelEma)
from libs.utils.badminton_metrics import BadmintonDetection
from libs.utils.badminton_validation import badminton_valid_one_epoch
from libs.utils.train_eval_utils import evaluate_on_training_set, create_train_evaluator
import numpy as np

# ç»˜å›¾åº“å¯¼å…¥
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.font_manager import FontProperties
import seaborn as sns
import pandas as pd

################################################################################
def plot_training_curves(all_eval_results, ckpt_folder, config_name):
    """
    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„losså’ŒmAPæ›²çº¿
    """
    print("\n" + "="*60)
    print("ðŸ“Š å³å°†ç»˜åˆ¶è®­ç»ƒæ›²çº¿å›¾è¡¨...")
    print("="*60)
    
    # ç¡®ä¿ä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Noto Sans CJK']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è®¾ç½®seabornæ ·å¼
    sns.set_style("whitegrid")
    
    # å‡†å¤‡æ•°æ®
    epochs = []
    train_losses = []
    val_losses = []
    train_map = []
    val_map = []
    val_class_ap = {}  # ç±»åˆ«APæ•°æ®
    
    # ä»Žç»“æžœä¸­æå–æ•°æ®
    for epoch_key, results in all_eval_results.items():
        epoch_num = int(epoch_key.split('_')[1])
        epochs.append(epoch_num)
        
        # è®­ç»ƒloss (å¦‚æžœæœ‰çš„è¯)
        if 'train_loss' in results:
            train_losses.append(results['train_loss'])
        else:
            train_losses.append(None)
            
        # éªŒè¯loss
        if 'validation_loss' in results:
            val_loss = results['validation_loss'].get('final_loss', 0.0)
            val_losses.append(val_loss)
        else:
            val_losses.append(None)
            
        # mAPæ•°æ®
        if 'results' in results:
            map_data = results['results']
            train_map.append(map_data.get('train_mAP@0.5', 0.0) * 100)  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            val_map.append(map_data.get('mAP@0.5', 0.0) * 100)
            
            # æå–ç±»åˆ«AP (ä½¿ç”¨æœ€åŽä¸€æ¬¡è¯„ä¼°çš„æ•°æ®)
            if 'AP_by_class' in map_data:
                for class_name, ap_value in map_data['AP_by_class'].items():
                    if class_name not in val_class_ap:
                        val_class_ap[class_name] = []
                    val_class_ap[class_name].append(ap_value * 100)
    
    if not epochs:
        print("âŒ æ²¡æœ‰è¯„ä¼°æ•°æ®å¯ç”¨äºŽç»˜å›¾")
        return
    
    # åˆ›å»ºå›¾è¡¨
    fig = plt.figure(figsize=(16, 12))
    
    # 1. Lossæ›²çº¿å›¾
    plt.subplot(2, 2, 1)
    if any(x is not None for x in train_losses):
        plt.plot(epochs, train_losses, 'b-', label='è®­ç»ƒLoss', linewidth=2, marker='o', markersize=4)
    if any(x is not None for x in val_losses):
        plt.plot(epochs, val_losses, 'r-', label='éªŒè¯Loss', linewidth=2, marker='s', markersize=4)
    
    plt.title('è®­ç»ƒå’ŒéªŒè¯Lossæ›²çº¿', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. mAPæ›²çº¿å›¾
    plt.subplot(2, 2, 2)
    if any(x > 0 for x in train_map):
        plt.plot(epochs, train_map, 'g-', label='è®­ç»ƒmAP@0.5', linewidth=2, marker='o', markersize=4)
    if any(x > 0 for x in val_map):
        plt.plot(epochs, val_map, 'purple', label='éªŒè¯mAP@0.5', linewidth=2, marker='s', markersize=4)
    
    plt.title('è®­ç»ƒå’ŒéªŒè¯mAP@0.5æ›²çº¿', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('mAP@0.5 (%)', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. å„åŠ¨ä½œç±»åˆ«APè¡¨çŽ° (æŸ±çŠ¶å›¾)
    plt.subplot(2, 2, 3)
    if val_class_ap:
        # ä½¿ç”¨æœ€åŽä¸€æ¬¡è¯„ä¼°çš„æ•°æ®
        last_epoch_ap = {}
        for class_name, ap_list in val_class_ap.items():
            if ap_list:
                last_epoch_ap[class_name] = ap_list[-1]
        
        if last_epoch_ap:
            classes = list(last_epoch_ap.keys())
            ap_values = list(last_epoch_ap.values())
            
            # æŒ‰APå€¼æŽ’åº
            sorted_pairs = sorted(zip(classes, ap_values), key=lambda x: x[1], reverse=True)
            classes_sorted, ap_values_sorted = zip(*sorted_pairs)
            
            # åˆ›å»ºæŸ±çŠ¶å›¾
            colors = plt.cm.viridis(np.linspace(0, 1, len(classes_sorted)))
            bars = plt.bar(range(len(classes_sorted)), ap_values_sorted, color=colors, alpha=0.7)
            
            plt.title('å„åŠ¨ä½œç±»åˆ«AP@0.5è¡¨çŽ°', fontsize=14, fontweight='bold')
            plt.xlabel('åŠ¨ä½œç±»åˆ«', fontsize=12)
            plt.ylabel('AP@0.5 (%)', fontsize=12)
            plt.xticks(range(len(classes_sorted)), classes_sorted, rotation=45, ha='right')
            plt.grid(True, alpha=0.3, axis='y')
            
            # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 4. è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    plt.subplot(2, 2, 4)
    plt.axis('off')
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    max_epoch = max(epochs) if epochs else 0
    best_val_map = max(val_map) if val_map else 0
    best_train_map = max(train_map) if train_map else 0
    final_train_map = train_map[-1] if train_map else 0
    final_val_map = val_map[-1] if val_map else 0
    
    stats_text = f"""
ðŸ“Š è®­ç»ƒç»Ÿè®¡ä¿¡æ¯

ðŸŽ¯ è®­ç»ƒé…ç½®:
   æ¨¡åž‹: {config_name}
   æ€»Epochs: {max_epoch}
   æ•°æ®é›†: ç¾½æ¯›çƒåŠ¨ä½œæ£€æµ‹

ðŸ“ˆ æ€§èƒ½è¡¨çŽ°:
   è®­ç»ƒmAP@0.5: {final_train_map:.1f}% (æœ€é«˜: {best_train_map:.1f}%)
   éªŒè¯mAP@0.5: {final_val_map:.1f}% (æœ€é«˜: {best_val_map:.1f}%)
   
ðŸ“‹ æœ€ä½³æ€§èƒ½:
   {'æ— æ•°æ®' if best_val_map == 0 else f'{best_val_map:.1f}%'}
   
ðŸ† TopåŠ¨ä½œ:
   {max(val_class_ap.items(), key=lambda x: x[1][-1] if x[1] else 0)[0] + f': {max(val_class_ap.items(), key=lambda x: x[1][-1] if x[1] else 0)[1][-1]:.1f}%' if val_class_ap and best_val_map > 0 else 'æ— æ•°æ®'}
"""
    
    plt.text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.1))
    
    # è®¾ç½®æ€»æ ‡é¢˜
    fig.suptitle(f'{config_name} - è®­ç»ƒæ›²çº¿åˆ†æž', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_file = os.path.join(ckpt_folder, 'training_curves.png')
    plt.savefig(plot_file, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"ðŸ“Š è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {plot_file}")
    
    # åŒæ—¶ä¿å­˜ä¸ºPDFæ ¼å¼
    plot_pdf_file = os.path.join(ckpt_folder, 'training_curves.pdf')
    plt.savefig(plot_pdf_file, bbox_inches='tight', facecolor='white')
    print(f"ðŸ“„ PDFç‰ˆæœ¬å·²ä¿å­˜åˆ°: {plot_pdf_file}")
    
    plt.close()
    print("âœ… è®­ç»ƒæ›²çº¿ç»˜åˆ¶å®Œæˆ!")

################################################################################
def main(args):
    """main function that handles training / inference with validation"""

    """1. setup parameters / folders"""
    # parse args
    args.start_epoch = 0
    if os.path.isfile(args.config):
        cfg = load_config(args.config)
    else:
        raise ValueError("Config file does not exist.")
    pprint(cfg)

    # prep for output folder (based on time stamp)
    if not os.path.exists(cfg['output_folder']):
        os.mkdir(cfg['output_folder'])
    cfg_filename = os.path.basename(args.config).replace('.yaml', '')
    if len(args.output) == 0:
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        ckpt_folder = os.path.join(
            cfg['output_folder'], cfg_filename + '_' + str(ts))
    else:
        ckpt_folder = os.path.join(
            cfg['output_folder'], cfg_filename + '_' + str(args.output))
    if not os.path.exists(ckpt_folder):
        os.mkdir(ckpt_folder)

    # create evaluation results folder
    eval_folder = os.path.join(ckpt_folder, 'eval_results')
    if not os.path.exists(eval_folder):
        os.mkdir(eval_folder)
        
    # tensorboard writer
    tb_writer = SummaryWriter(os.path.join(ckpt_folder, 'logs'))

    # fix the random seeds (this will fix everything)
    rng_generator = fix_random_seed(cfg['init_rand_seed'], include_cuda=True)

    # re-scale learning rate / # workers based on number of GPUs
    cfg['opt']["learning_rate"] *= len(cfg['devices'])
    cfg['loader']['num_workers'] *= len(cfg['devices'])

    """2. create dataset / dataloader"""
    train_dataset = make_dataset(
        cfg['dataset_name'], True, cfg['train_split'], **cfg['dataset']
    )
    # update cfg based on dataset attributes (fix to epic-kitchens)
    train_db_vars = train_dataset.get_attributes()
    cfg['model']['train_cfg']['head_empty_cls'] = train_db_vars['empty_label_ids']

    # data loaders
    train_loader = make_data_loader(
        train_dataset, True, rng_generator, **cfg['loader'])

    # validation dataset and loader
    val_dataset = make_dataset(
        cfg['dataset_name'], False, cfg['val_split'], **cfg['dataset']
    )
    val_loader = make_data_loader(
        val_dataset, False, None, 1, cfg['loader']['num_workers'])

    """3. create model, optimizer, and scheduler"""
    # model
    model = make_meta_arch(cfg['model_name'], **cfg['model'])
    # not ideal for multi GPU training, ok for now
    model = nn.DataParallel(model, device_ids=cfg['devices'])
    # optimizer
    optimizer = make_optimizer(model, cfg['opt'])
    # schedule
    num_iters_per_epoch = len(train_loader)
    scheduler = make_scheduler(optimizer, cfg['opt'], num_iters_per_epoch)

    # enable model EMA
    print("Using model EMA ...")
    model_ema = ModelEma(model)

    """4. Resume from model / Misc"""
    # resume from a checkpoint?
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            checkpoint = torch.load(
                args.resume,
                map_location = lambda storage, loc: storage.cuda(
                    cfg['devices'][0]))
            args.start_epoch = checkpoint['epoch']
            model.load_state_dict(checkpoint['state_dict'])
            model_ema.module.load_state_dict(checkpoint['state_dict_ema'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            scheduler.load_state_dict(checkpoint['scheduler'])
            print("=> loaded checkpoint '{}' (epoch {})".format(
                args.resume, checkpoint['epoch']
            ))
            del checkpoint
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))
            return

    # save the current config
    with open(os.path.join(ckpt_folder, 'config.txt'), 'w') as fid:
        pprint(cfg, stream=fid)
        fid.flush()

    """5. training / validation loop"""
    print("\nStart training model {:s} ...".format(cfg['model_name']))
    
    # create evaluator for validation - ä½¿ç”¨ç¾½æ¯›çƒä¸“ç”¨çš„è¯„ä¼°å™¨
    val_db_vars = val_dataset.get_attributes()
    try:
        # å°è¯•ä½¿ç”¨æ ‡å‡†çš„ANETdetection
        det_eval = ANETdetection(
            val_dataset.json_file,
            val_dataset.split[0],
            tiou_thresholds = val_db_vars['tiou_thresholds']
        )
        print("Using ANETdetection evaluator")
    except KeyError:
        # å¦‚æžœå¤±è´¥ï¼Œä½¿ç”¨ç¾½æ¯›çƒä¸“ç”¨çš„è¯„ä¼°å™¨
        det_eval = BadmintonDetection(
            val_dataset.json_file,
            split=val_dataset.split[0],
            tiou_thresholds = val_db_vars['tiou_thresholds']
        )
        print("Using BadmintonDetection evaluator")
    
    # training loop with validation
    max_epochs = cfg['opt']['warmup_epochs'] + cfg['opt']['epochs']
    
    # store all evaluation results
    all_eval_results = {}
    
    # åˆ›å»ºè®­ç»ƒé›†è¯„ä¼°å™¨
    train_evaluator = None
    if 'train_eval_split' in cfg:
        print("åˆ›å»ºè®­ç»ƒé›†è¯„ä¼°å™¨...")
        train_evaluator = create_train_evaluator(cfg, train_dataset)
    
    # ç¡®å®šprint_freqï¼šä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼Œå…¶æ¬¡ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    print_freq = cfg.get('print_freq', args.print_freq)
    print(f"Lossæ‰“å°é¢‘çŽ‡: æ¯ {print_freq} ä¸ªbatchæ‰“å°ä¸€æ¬¡")
    
    for epoch in range(args.start_epoch, max_epochs):
        # train for one epoch
        train_one_epoch(
            train_loader,
            model,
            optimizer,
            scheduler,
            epoch,
            model_ema = model_ema,
            clip_grad_l2norm = cfg['train_cfg']['clip_grad_l2norm'],
            tb_writer=tb_writer,
            print_freq=print_freq
        )

        # è®­ç»ƒé›†å†…éƒ¨è¯„ä¼°
        if train_evaluator is not None and (epoch + 1) % args.eval_freq == 0:
            print(f"\n=> è®­ç»ƒé›†å†…éƒ¨è¯„ä¼° at epoch {epoch + 1}")
            # èŽ·å–max_batchesé…ç½®
            max_batches = cfg.get('train_eval_cfg', {}).get('max_batches', None)
            train_eval_results = evaluate_on_training_set(
                model_ema.module,
                train_loader,
                train_evaluator,
                epoch + 1,
                ckpt_folder,
                max_batches=max_batches
            )
            
            # è®°å½•åˆ°tensorboard
            if 'label' in train_eval_results:
                for tiou in [0.1, 0.2, 0.3, 0.4, 0.5]:
                    if tiou in train_eval_results:
                        ap_values = train_eval_results[tiou]
                        map_val = np.mean(ap_values)
                        tb_writer.add_scalar(f'Train/mAP@{tiou:.1f}', map_val, epoch)

        # validation every few epochs
        if (epoch + 1) % args.eval_freq == 0 or (epoch + 1) == max_epochs:
            print(f"\n=> Evaluating at epoch {epoch + 1}")
            
            # validation output file for this epoch
            eval_file = os.path.join(eval_folder, f'eval_results_epoch_{epoch+1:03d}.json')
            
            # run validation - ä½¿ç”¨ç¾½æ¯›çƒä¸“ç”¨çš„éªŒè¯å‡½æ•°
            if isinstance(det_eval, BadmintonDetection):
                eval_results = badminton_valid_one_epoch(
                    val_loader,
                    model_ema.module,
                    epoch,
                    evaluator=det_eval,
                    output_file=eval_file,
                    tb_writer=tb_writer,
                    print_freq=args.print_freq,
                    compute_loss=True  # è®¡ç®—éªŒè¯é›†loss
                )
            else:
                eval_results = valid_one_epoch(
                    val_loader,
                    model_ema.module,
                    epoch,
                    evaluator=det_eval,
                    output_file=eval_file,
                    tb_writer=tb_writer,
                    print_freq=args.print_freq
                )

            print(eval_results['validation_loss'])
            print(eval_results["results"])

            # read and store evaluation results
            if os.path.exists(eval_file):
                with open(eval_file, 'r', encoding='utf-8') as f:
                    eval_results = json.load(f)
                    all_eval_results[f'epoch_{epoch+1:03d}'] = eval_results
                    
                    # log to tensorboard
                    if 'results' in eval_results:
                        for key, value in eval_results['results'].items():
                            if isinstance(value, (int, float)):
                                tb_writer.add_scalar(f'Validation/{key}', value, epoch)
                    
                    # log validation loss to tensorboard
                    if 'validation_loss' in eval_results:
                        for key, value in eval_results['validation_loss'].items():
                            if isinstance(value, (int, float)):
                                tb_writer.add_scalar(f'Validation_Loss/{key}', value, epoch)
                    
                    print(f"Validation results saved to: {eval_file}")

        # save checkpoint
        if (
            ((epoch + 1) == max_epochs) or
            ((args.ckpt_freq > 0) and ((epoch + 1) % args.ckpt_freq == 0))
        ):
            save_states = {
                'epoch': epoch + 1,
                'state_dict': model.state_dict(),
                'scheduler': scheduler.state_dict(),
                'optimizer': optimizer.state_dict(),
            }

            save_states['state_dict_ema'] = model_ema.module.state_dict()
            save_checkpoint(
                save_states,
                False,
                file_folder=ckpt_folder,
                file_name='epoch_{:03d}.pth.tar'.format(epoch + 1)
            )

    # save all evaluation results summary
    summary_file = os.path.join(eval_folder, 'all_eval_results.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(all_eval_results, f, indent=2, ensure_ascii=False)
    print(f"\nAll evaluation results saved to: {summary_file}")
    
    # find best epoch
    best_epoch = None
    best_map = 0.0
    for epoch_key, results in all_eval_results.items():
        if 'results' in results and 'mAP' in results['results']:
            current_map = results['results']['mAP']
            if current_map > best_map:
                best_map = current_map
                best_epoch = epoch_key
    
    if best_epoch:
        print(f"Best epoch: {best_epoch} with mAP: {best_map:.4f}")
        # save best epoch info
        best_info = {
            'best_epoch': best_epoch,
            'best_mAP': best_map,
            'best_results': all_eval_results[best_epoch]
        }
        with open(os.path.join(eval_folder, 'best_results.json'), 'w', encoding='utf-8') as f:
            json.dump(best_info, f, indent=2, ensure_ascii=False)

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    config_name = os.path.basename(cfg_filename) if cfg_filename else "unknown_config"
    plot_training_curves(all_eval_results, ckpt_folder, config_name)

    # wrap up
    tb_writer.close()
    print("All done!")
    return

################################################################################
if __name__ == '__main__':
    """Entry Point"""
    # the arg parser
    parser = argparse.ArgumentParser(
      description='Train a point-based transformer for action localization with validation')
    parser.add_argument('config', metavar='DIR',
                        help='path to a config file')
    parser.add_argument('-p', '--print-freq', default=10, type=int,
                        help='print frequency (default: 10 iterations)')
    parser.add_argument('-c', '--ckpt-freq', default=5, type=int,
                        help='checkpoint frequency (default: every 5 epochs)')
    parser.add_argument('-e', '--eval-freq', default=5, type=int,
                        help='evaluation frequency (default: every 5 epochs)')
    parser.add_argument('--output', default='', type=str,
                        help='name of exp folder (default: none)')
    parser.add_argument('--resume', default='', type=str, metavar='PATH',
                        help='path to a checkpoint (default: none)')
    args = parser.parse_args()
    main(args)
